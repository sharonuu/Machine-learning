# 线性回归的概念
## 线性回归的一般形式
有数据集 {(𝑥1,𝑦1),(𝑥2,𝑦2),...,(𝑥𝑛,𝑦𝑛)} ,其中, 𝑥𝑖=(𝑥𝑖1;𝑥𝑖2;𝑥𝑖3;...;𝑥𝑖𝑑),𝑦𝑖∈𝑅 
其中n表示变量的数量，d表示每个变量的维度。
可以用以下函数来描述y和x之间的关系：

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020011121305513.png#pic_center=200x65)

并且常用均方误差作为回归中常用的性能度量

## 线性回归相关函数

 - 损失函数(Loss Function)：度量单样本预测的错误程度，损失函数值越小，模型就越好。
 - 代价函数(Cost Function)：度量全部样本集的平均误差。
 - 目标函数(Object Function)：代价函数和正则化函数，最终要优化的函数。
 

## 优化方法
生成数据

```
import numpy as np
#生成随机数
np.random.seed(1234)
x = np.random.rand(500,3)
#构建映射关系，模拟真实的数据待预测值,映射关系为y = 4.2 + 5.7*x1 + 10.8*x2，可自行设置值进行尝试
y = x.dot(np.array([4.2,5.7,10.8]))
```

### 梯度下降法

```
class LR_GD():
    def _init_(self):
        self.w = None
    def fit(self,X,y,alpha = 0.02,loss = 1e-10):
        y = y.reshape(-1,1)
        [m,d] = np.shape(X)
        self.w = np.zeros((d))
        tol = 1e5
        if tol >loss:
            told= y-np.dot(X,self.w)
            self.w = self.w - 1/m *alpha * (np.dot(told , X))
            tol = np.abs(np.sum(told))
    def predict(self,X):
        y_pred = np.dot(X,self.w)
        return y_pred
if __name__ == "__main__":
    lr_gd = LR_GD()
    lr_gd.fit(x,y)
    print("估计参数值为：%s"%(lr_gd.w))
    x_test = np.array([2,4,5]).reshape(1,-1)
    print("预测值:%s"%(lr_gd.predict(x_test)))

```

### 最小二乘法矩阵求解

```
class LR_LS():
    def _init_(self):
        self.w = None
    def fit(self,X,y):
        temp0 = np.dot(X.T,X)
        temp = np.dot(np.linalg.inv(temp0),X.T)
        self.w = np.dot(temp,y)
    def predict(self,X):
        y_pred = np.dot(X,self.w)
        return y_pred
if __name__ == "__main__":
    lr_ls = LR_LS()
    lr_ls.fit(x,y)
    print("估计的参数值：%s"%(lr_ls.w))
    x_test = np.array([2,4,5]).reshape(1,-1)
    print("预测值为：%s"%(lr_ls.predict(x_test)))
```

## 线性回归的评价指标
均方误差、均方根误差、平均绝对误差、R平方
