## sklearn接口

```
from sklearn.naive_bayes import GaussianNB
from sklearn.datasets import load_iris
import pandas as pd
from sklearn.model_selection import train_test_split
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2)
clf = GaussianNB().fit(X_train, y_train)
print ("Classifier Score:", clf.score(X_test, y_test))
```
<table>
<thead>
<tr>
  <th align="center">编号</th>
  <th align="center">色泽</th>
  <th align="center">根蒂</th>
  <th align="center">敲声</th>
  <th align="center">纹理</th>
  <th align="center">脐部</th>
  <th align="center">触感</th>
  <th align="center">好瓜</th>
</tr>
</thead>
<tbody><tr>
  <td align="center">1</td>
  <td align="center">青绿</td>
  <td align="center">蜷缩</td>
  <td align="center">浊响</td>
  <td align="center">清晰</td>
  <td align="center">凹陷</td>
  <td align="center">硬滑</td>
  <td align="center">是</td>
</tr>
<tr>
  <td align="center">2</td>
  <td align="center">乌黑</td>
  <td align="center">蜷缩</td>
  <td align="center">沉闷</td>
  <td align="center">清晰</td>
  <td align="center">凹陷</td>
  <td align="center">硬滑</td>
  <td align="center">是</td>
</tr>
<tr>
  <td align="center">3</td>
  <td align="center">乌黑</td>
  <td align="center">蜷缩</td>
  <td align="center">浊响</td>
  <td align="center">清晰</td>
  <td align="center">凹陷</td>
  <td align="center">硬滑</td>
  <td align="center">是</td>
</tr>
<tr>
  <td align="center">4</td>
  <td align="center">青绿</td>
  <td align="center">蜷缩</td>
  <td align="center">沉闷</td>
  <td align="center">清晰</td>
  <td align="center">凹陷</td>
  <td align="center">硬滑</td>
  <td align="center">是</td>
</tr>
<tr>
  <td align="center">5</td>
  <td align="center">浅白</td>
  <td align="center">蜷缩</td>
  <td align="center">浊响</td>
  <td align="center">清晰</td>
  <td align="center">凹陷</td>
  <td align="center">硬滑</td>
  <td align="center">是</td>
</tr>
<tr>
  <td align="center">6</td>
  <td align="center">青绿</td>
  <td align="center">稍蜷</td>
  <td align="center">浊响</td>
  <td align="center">清晰</td>
  <td align="center">稍凹</td>
  <td align="center">软粘</td>
  <td align="center">是</td>
</tr>
<tr>
  <td align="center">7</td>
  <td align="center">乌黑</td>
  <td align="center">稍蜷</td>
  <td align="center">浊响</td>
  <td align="center">稍糊</td>
  <td align="center">稍凹</td>
  <td align="center">软粘</td>
  <td align="center">是</td>
</tr>
<tr>
  <td align="center">8</td>
  <td align="center">乌黑</td>
  <td align="center">稍蜷</td>
  <td align="center">浊响</td>
  <td align="center">清晰</td>
  <td align="center">稍凹</td>
  <td align="center">硬滑</td>
  <td align="center">是</td>
</tr>
<tr>
  <td align="center">9</td>
  <td align="center">乌黑</td>
  <td align="center">稍蜷</td>
  <td align="center">沉闷</td>
  <td align="center">稍糊</td>
  <td align="center">稍凹</td>
  <td align="center">硬滑</td>
  <td align="center">否</td>
</tr>
<tr>
  <td align="center">10</td>
  <td align="center">青绿</td>
  <td align="center">硬挺</td>
  <td align="center">清脆</td>
  <td align="center">清晰</td>
  <td align="center">平坦</td>
  <td align="center">软粘</td>
  <td align="center">否</td>
</tr>
<tr>
  <td align="center">11</td>
  <td align="center">浅白</td>
  <td align="center">硬挺</td>
  <td align="center">清脆</td>
  <td align="center">模糊</td>
  <td align="center">平坦</td>
  <td align="center">硬滑</td>
  <td align="center">否</td>
</tr>
<tr>
  <td align="center">12</td>
  <td align="center">浅白</td>
  <td align="center">蜷缩</td>
  <td align="center">浊响</td>
  <td align="center">模糊</td>
  <td align="center">平坦</td>
  <td align="center">软粘</td>
  <td align="center">否</td>
</tr>
<tr>
  <td align="center">13</td>
  <td align="center">青绿</td>
  <td align="center">稍蜷</td>
  <td align="center">浊响</td>
  <td align="center">稍糊</td>
  <td align="center">凹陷</td>
  <td align="center">硬滑</td>
  <td align="center">否</td>
</tr>
<tr>
  <td align="center">14</td>
  <td align="center">浅白</td>
  <td align="center">稍蜷</td>
  <td align="center">沉闷</td>
  <td align="center">稍糊</td>
  <td align="center">凹陷</td>
  <td align="center">硬滑</td>
  <td align="center">否</td>
</tr>
<tr>
  <td align="center">15</td>
  <td align="center">乌黑</td>
  <td align="center">稍蜷</td>
  <td align="center">浊响</td>
  <td align="center">清晰</td>
  <td align="center">稍凹</td>
  <td align="center">软粘</td>
  <td align="center">否</td>
</tr>
<tr>
  <td align="center">16</td>
  <td align="center">浅白</td>
  <td align="center">蜷缩</td>
  <td align="center">浊响</td>
  <td align="center">模糊</td>
  <td align="center">平坦</td>
  <td align="center">硬滑</td>
  <td align="center">否</td>
</tr>
<tr>
  <td align="center">17</td>
  <td align="center">青绿</td>
  <td align="center">蜷缩</td>
  <td align="center">沉闷</td>
  <td align="center">稍糊</td>
  <td align="center">稍凹</td>
  <td align="center">硬滑</td>
  <td align="center">否</td>
</tr>
</tbody></table>
## 贝叶斯决策理论
贝叶斯决策论是概率框架下实施决策的基本方法，对分类任务来说，在所有相关概率都已知的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记。

假设有N种可能标记， $λ_{ij}$是将类$c_j$误分类为$c_i$所产生的损失，基于后验概率$ P(c_i | x)$ 可以获得样本x分类为$c_i$所产生的期望损失 ，即在样本x上的条件风险：

$$R(c_i|\mathbf{x}) = \sum_{j=1}^N \lambda_{ij} P(c_j|\mathbf{x})$$
我们的任务是寻找一个判定准则 $h:X→Y$以最小化总体风险

$$R(h)= \mathbb{E}_x [R(h(\mathbf(x)) | \mathbf(x))]$$

显然，对每个样本x，若h能最小化条件风险 $R(h((x))|(x))$,则总体风险R(h)也将被最小化。这就产生了贝叶斯判定准则：为最小化总体风险，只需要在每个样本上选择那个能使条件风险R(c|x)最小的类别标记，即：

$$h^* (x) = argmin_{c\in y} R(c|\mathbf{x})$$
此时，h 称作贝叶斯最优分类器，与之对应的总体风险R(h )称为贝叶斯风险，1-R(h*)反映了分类器能达到的最好性能，即机器学习所产生的模型精度的上限。

具体来说，若目标是最小化分类错误率（对应0/1损失），则$λ_{ij}$可以用0/1损失改写，得到条件风险和最小化分类错误率的最优分类器分别为：
$$R(c|\mathbf{x}) = 1- P(c|\mathbf{x})$$

$$h^*(x) = argmax_{c\in \mathcal{Y}} P(c|\mathbf{x})$$

即对每个样本x，选择能使后验概率P(c|x)最大的类别标识。

获得后验概率的两种方法：

1. 判别式模型:给定x，可以通过直接建模P(c|x)来预测c。
2. 生成模型:先对联合分布p(x, c)建模，然后再有此获得P(c|x)。
## 贝叶斯公式
对生成模型来说，必然考虑：
$$P(c|x) = \frac{P(x,c)}{P(x)} = \frac{P(c) P(x|c)}{P(x)}$$
其中P(c)是“先验概率”；

P(x|c)是样本x对于类标记c的类条件概率，或称为“似然”；

P(x)是用于归一化的“证据”因子。

上式即为贝叶斯公式。

可以将其看做$$P(类别|特征) = \frac{P(特征,类别)}{P(特征)} = \frac{P(类别) P(特征|类别)}{P(特征)}$$

对类条件概率P(x|c)来说，直接根据样本出现的频率来估计将会遇到严重的困难，所以引入了极大似然估计。
贝叶斯分类器的训练过程就是参数估计。总结最大似然法估计参数的过程，一般分为以下四个步骤：
```
1.写出似然函数；
2.对似然函数取对数，并整理；
3.求导数，令偏导数为0，得到似然方程组；
4.解似然方程组，得到所有参数即为所求。
```
### 朴素贝叶斯分类器
基于贝叶斯公式来估计后验概率P(c|x)主要困难在于类条件概率P(x|c)是所有属性上的联合概率，难以从有限的训练样本直接估计而得。
基于有限训练样本直接计算联合概率，在计算上将会遭遇组合爆炸问题；在数据上将会遭遇样本稀疏问题；属性越多，问题越严重。

为了避开这个障碍，朴素贝叶斯分类器采用了$“属性条件独立性假设”$:对已知类别，假设所有属性相互独立。换言之，假设每个属性独立的对分类结果发生影响相互独立。

回答西瓜的例子就可以认为｛色泽	根蒂	敲声	纹理	脐部	触感｝这些属性对西瓜是好还是坏的结果所产生的影响相互独立。

基于条件独立性假设，对于多个属性的后验概率可以写成：
$$P(c|\mathbf{x}) = \frac{P(C)P(\mathbf{x}|c)}{P(\mathbf{x})} = \frac{P(c)}{P(\mathbf{x})}\prod_{i=1}^d P(x_i|c)$$

d为属性数目，$x_i$是x在第i个属性上取值。
对于所有的类别来说P(x)相同，基于极大似然的贝叶斯判定准则有朴素贝叶斯的表达式：
$$h_{nb}(\mathbf{x}) = \arg max_{c\in \mathcal{Y}}P(c)\prod_{i=1}^d P(x_i|c) \quad (1)$$

## sklearn参数详解
1. 高斯朴素贝叶斯算法是假设特征的可能性(即概率)为高斯分布。
```
class sklearn.naive_bayes.GaussianNB(priors=None)
参数：
priors:先验概率大小，如果没有给定，模型则根据样本数据自己计算（利用极大似然法）。
var_smoothing：可选参数，所有特征的最大方差
属性：
class_prior_:每个样本的概率
class_count:每个类别的样本数量
classes_:分类器已知的标签类型
theta_:每个类别中每个特征的均值
sigma_:每个类别中每个特征的方差
epsilon_:方差的绝对加值方法
# 贝叶斯的方法和其他模型的方法一致。
fit(X,Y):在数据集(X,Y)上拟合模型。
get_params():获取模型参数。
predict(X):对数据集X进行预测。
predict_log_proba(X):对数据集X预测，得到每个类别的概率对数值。predict_proba(X):对数据集X预测，得到每个类别的概率。
score(X,Y):得到模型在数据集(X,Y)的得分情况。
```
代码：

```
import math
class NaiveBayes:
    def __init__(self):
        self.model = None

    # 数学期望
    @staticmethod
    def mean(X):
        """计算均值
        Param: X : list or np.ndarray
        
        Return:
            avg : float
        
        """
        avg = 0.0
        avg = sum(X)/float(len(X))
        return avg

    # 标准差（方差）
    def stdev(self, X):
        """计算标准差
        Param: X : list or np.ndarray
        
        Return:
            res : float
        
        """
        res = 0.0
        avg = self.mean(X)
        res = math.sqrt(sum([pow(x-avg,2)for x in X])/float(len(X)))
        return res
        
    # 概率密度函数
    def gaussian_probability(self, x, mean, stdev):
        """根据均值和标注差计算x符号该高斯分布的概率
        Parameters:
        ----------
        x : 输入
        mean : 均值
        stdev : 标准差
        
        Return:
        
        res : float， x符合的概率值
            
        """
        res = 0.0
        exponent = math.exp(-(math.pow(x-mean,2)/(2*pow(stdev,2))))
        
        return res
        
    # 处理X_train
    def summarize(self, train_data):
        """计算每个类目下对应数据的均值和标准差
        Param: train_data : list
        
        Return : [mean, stdev]
        """
        summaries = [0.0, 0.0]
        summaries = [(self.mean(i),self.stdev(i))for i in zip(*train_data)]
        return summaries

    # 分类别求出数学期望和标准差
    def fit(self, X, y):
        labels = list(set(y))
        data = {label: [] for label in labels}
        for f, label in zip(X, y):
            data[label].append(f)
        self.model = {
            label: self.summarize(value) for label, value in data.items()
        }
        return 'gaussianNB train done!'

    # 计算概率
    def calculate_probabilities(self, input_data):
        """计算数据在各个高斯分布下的概率
        Paramter:
        input_data : 输入数据
        
        Return:
        probabilities : {label : p}
        """
        # summaries:{0.0: [(5.0, 0.37),(3.42, 0.40)], 1.0: [(5.8, 0.449),(2.7, 0.27)]}
        # input_data:[1.1, 2.2]
        probabilities = {}
       
        for label, value in self.model.items():
            probabilities[label] = 1
            for i in range(len(value)):
                mean, stdev = value[i]
                probabilities[label] *= self.gaussian_probability(input_data[i], mean, stdev)
            
  
        return probabilities

    # 类别
    def predict(self, X_test):
        # {0.0: 2.9680340789325763e-27, 1.0: 3.5749783019849535e-26}
        label = sorted(self.calculate_probabilities(X_test).items(), key=lambda x: x[-1])[-1][0]
        return label
    # 计算得分
    def score(self, X_test, y_test):
        right = 0
        for X, y in zip(X_test, y_test):
            label = self.predict(X)
            if label == y:
                right += 1

        return right / float(len(X_test))
```
