# 机器学习概述
## 基本概念
机器学习是一门多领域交叉学科，涉及概率论、统计学、、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径.

## 机器学习分类
### 1.监督学习
监督学习是指利用一组**已知类别的样本调整分类器的参数**，使其达到所要求性能的过程，也称为监督训练或有教师学习。在监督学习的过程中会提供**对错指示**，通过不断地重复训练，使其找到给定的训练数据集中的**某种模式或规律**，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求包括输入和输出，主要应用于**分类和预测。**
### 2.半监督学习
与监督学习不同，在非监督学习中，无须对数据集进行标记，即没有输出。其需要从数据集中发现隐含的某种结构，从而获得**样本数据的结构特征**，判断哪些数据比较相似。因此，非监督学习目标不是告诉计算机怎么做，而是让它去**学习怎样做事情**。
### 3.半监督学习
半监督学习是监督学习和非监督学习的结合，其在训练阶段使用的是**未标记的数据和已标记的数据**，不仅要学习属性之间的结构关系，也要输出分类模型进行预测。
### 4.强化学习
强化学习（Reinforcement Learning, RL），又称再励学习、评价学习或增强学习，是机器学习的范式和方法论之一，用于描述和解决智能体（agent）在与环境的交互过程中通过学习策略以**达成回报最大化或实现特定目标的问题**.

## 机器学习模型
机器学习 = 数据（data） + 模型（model） + 优化方法（optimal strategy）
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200108221053712.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc5OTQxOQ==,size_16,color_FFFFFF,t_70)
## 常见的机器学习算法
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200108222110565.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc5OTQxOQ==,size_16,color_FFFFFF,t_70)

## 损失函数
1.0-1损失函数$$ L(y,f(x)) = \begin{cases} 0, & \text{y = f(x)} \ 1, & \text{y $\neq$ f(x)} \end{cases} $$
2.绝对值损失函数$$ L(y,f(x))=|y-f(x)| $$
3.平方损失函数 $$ L(y,f(x))=(y-f(x))^2 $$
4.log对数损失函数 $$ L(y,f(x))=log(1+e^{-yf(x)}) $$
5.指数损失函数$$ L(y,f(x))=exp(-yf(x)) $$
6.hinge损失函数$$ L(w,b)=max{0,1-yf(x)} $$

## 机器学习优化方法
梯度下降是最常用的优化方法之一，它使用梯度的反方向$\nabla_\theta J(\theta)$更新参数$\theta$，使得目标函数$J(\theta)$达到最小化的一种优化方法，这种方法我们叫做梯度更新.

(全量)梯度下降 $$ \theta=\theta-\eta\nabla_\theta J(\theta) $$
随机梯度下降 $$ \theta=\theta-\eta\nabla_\theta J(\theta;x^{(i)},y^{(i)}) $$
小批量梯度下降 $$ \theta=\theta-\eta\nabla_\theta J(\theta;x^{(i:i+n)},y^{(i:i+n)}) $$
引入动量的梯度下降 $$ \begin{cases} v_t=\gamma v_{t-1}+\eta \nabla_\theta J(\theta) \ \theta=\theta-v_t \end{cases} $$
自适应学习率的Adagrad算法 $$ \begin{cases} g_t= \nabla_\theta J(\theta) \ \theta_{t+1}=\theta_{t,i}-\frac{\eta}{\sqrt{G_t+\varepsilon}} \cdot g_t \end{cases} $$
牛顿法 $$ \theta_{t+1}=\theta_t-H^{-1}\nabla_\theta J(\theta_t) $$ 其中: $t$: 迭代的轮数
$\eta$: 学习率

$G_t$: 前t次迭代的梯度和

$\varepsilon:$很小的数,防止除0错误

$H$: 损失函数相当于$\theta$的Hession矩阵在$\theta_t$处的估计

## 机器学习的评价指标
1.MSE(Mean Squared Error) $$ MSE(y,f(x))=\frac{1}{N}\sum_{i=1}^{N}(y-f(x))^2 $$
2.MAE(Mean Absolute Error) $$ MSE(y,f(x))=\frac{1}{N}\sum_{i=1}^{N}|y-f(x)| $$
3.RMSE(Root Mean Squard Error) $$ RMSE(y,f(x))=\frac{1}{1+MSE(y,f(x))} $$
4.Top-k准确率 $$ Top_k(y,pre_y)=\begin{cases} 1, {y \in pre_y} \ 0, {y \notin pre_y} \end{cases} $$
5.混淆矩阵

## 机器学习模型选择
1.交叉验证
所有数据分为三部分：训练集、交叉验证集和测试集。交叉验证集不仅在选择模型时有用，在超参数选择、正则项参数 [公式] 和评价模型中也很有用。
2.k-折叠交叉验证
 - 假设训练集为S ，将训练集等分为k份:${S_1, S_2, ..., S_k}$.
 - 然后每次从集合中拿出k-1份进行训练
 - 利用集合中剩下的那一份来进行测试并计算损失值
 - 最后得到k次测试得到的损失值，并选择平均损失值最小的模型
3.Bias与Variance，欠拟合与过拟合
欠拟合一般表示模型对数据的表现能力不足，通常是模型的复杂度不够，并且Bias高，训练集的损失值高，测试集的损失值也高.

过拟合一般表示模型对数据的表现能力过好，通常是模型的复杂度过高，并且Variance高，训练集的损失值低，测试集的损失值高.
4.解决方法
- 增加训练样本: 解决高Variance情况
- 减少特征维数: 解决高Variance情况
- 减小模型复杂度: 解决高Variance情况
- 增加特征维数: 解决高Bias情况
- 增加模型复杂度: 解决高Bias情况

## 机器学习参数调优
1.网格搜索
一种调参手段；穷举搜索：在所有候选的参数选择中，通过循环遍历，尝试每一种可能性，表现最好的参数就是最终的结果

2.随机搜索
与网格搜索相比，随机搜索并未尝试所有参数值，而是从指定的分布中采样固定数量的参数设置。它的理论依据是，如果随即样本点集足够大，那么也可以找到全局的最大或最小值，或它们的近似值。通过对搜索范围的随机取样，随机搜索一般会比网格搜索要快一些。

3.贝叶斯优化算法
贝叶斯优化用于机器学习调参由J. Snoek(2012)提出，主要思想是，给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。
