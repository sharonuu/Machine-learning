# 逻辑回归与线性回归区别
线性回归多解决连续变量问题，在分类问题中效果不好

# 逻辑回归原理
对于分类模型常需要当为1的概率大于0.5时，判断为1，当为1的概率小于0.5时，判断为0。因概率的值域为 [0,1] ，这样的设定比线性回归合理很多。因此，常用函数sigmoid函数：
 $$
h(z) = \frac{1}{1+e^{-z}}
$$
其中，$z = \theta^T x$
# 损失函数
$$
J(\theta) = -\frac{1}{m}l(\theta) = -\frac{1}{m}\sum^{m}_{i=1} y^{(i)}h_\theta (x^{(i)}) + (1-y^{(i)})(1-h_\theta (x^{(i)}))
$$
假如和线性回归一样的平方损失函数，则损失函数的形式为$\sum^m_{i=1}(y^{(i)}-\frac{1}{1+e^{-\theta^T x}})^2$，此为非凸函数，求解复杂，且容易求得局部最优解为非全局最优解。

# 逻辑回归的分布式实现
由于单机处理能力的限制，在对大规模样本训练时，往往需要将求解过程并行化。我们知道在求解过程中，行和列都存在批向量处理的情况，我们可以按行并行和按列并行。
### 按行并行
看$\frac{\partial J(\theta)}{\partial \theta_i} =\frac{1}{m}\sum^m_{i=1}(y^{(i)} - h_\theta (x^{(i)}))x^{(i)}$，相当于将上个迭代步骤结果，将每个样本进行运算后求和，那么只需将样本拆分到不同机器上运算，最后在求和即可。
### 按列并行
$\frac{\partial J(\theta)}{\partial \theta_i} =\frac{1}{m}\sum^m_{i=1}(y^{(i)} - h_\theta (x^{(i)}))x^{(i)}$是对参数$\theta_i$进行的运算，那么只需将特征分配到不同的机器上，分别计算梯度后，再归并到完整的特征向量进行梯度更新即可。
# 正则化与模型评估指标
### 正则化
我们可以在损失函数后面，加上正则化函数，即 𝜃 的惩罚项，来抑制过拟合问题。
#### L1正则:
$$
J(\theta) =\frac{1}{m}\sum^{m}_{i=1} y^{(i)}h_\theta (x^{(i)}) + (1-y^{(i)})(1-h_\theta (x^{(i)})) + \frac{\lambda}{m
}\sum^m_{i=1}|\theta_i|
$$
$$
\Delta_{\theta_i} l(\theta) = \frac{1}{m}\sum^m_{i=1}(y^{(i)} - h_\theta (x^{(i)}))x^{(i)} + \frac{\lambda}{m}sgn(\theta_i)
$$
梯度下降法的迭代函数变为,
$$\theta:=\theta-K'(\theta)-\frac{\lambda}{m}sgn(\theta)$$
$K(\theta)$为原来的损失函数，由于最后一项的符号由$\theta$决定，可以看到，当$\theta$大于零时，更新后的$\theta$变小；当$\theta$小于零时，更新后的$\theta$变大。因此，L1正则化调整后的结果会更加稀疏（结果向量中有更多的0值）。（见图示，相当于在等高线上找令菱形最小的点。）
#### L2正则
$$
J(\theta) =\frac{1}{m}\sum^{m}_{i=1} y^{(i)}h_\theta (x^{(i)}) + (1-y^{(i)})(1-h_\theta (x^{(i)})) + \frac{\lambda}{2m}\sum^m_{i=1}\theta_i^2
$$
$$
\Delta_{\theta_i} l(\theta) = \frac{1}{m}\sum^m_{i=1}(y^{(i)} - h_\theta (x^{(i)}))x^{(i)} + \frac{\lambda}{m}\theta_i
$$
梯度下降法的迭代函数变为,
$$\theta:=\theta-K'(\theta)-\frac{2\lambda}{m}\theta$$
$K(\theta)$为原来的损失函数，最有一项的$\lambda$决定了对参数的惩罚力度，惩罚力度越大，最后的结果向量的参数普遍较小且分散，避免了个别参数对整个函数起较大的影响。（见图示，相当于在等高线上找令圆形最小的点）
#### 逻辑回归的评价指标
由于逻辑回归模型属于分类模型，不能用线性回归的评价指标。
二元分类的评价指标基本都适用于逻辑回归。
我们可以用查准率和查全率来评价预测结果：
* 查准率 $P =\frac{TP}{TP+FP}$ 
* 查全率 $R =\frac{TP}{TP+FN}$
我们可以用$F_\beta$表达对查准率/查全率的不同偏好，定义为：
$$F_\beta = \frac{(1+\beta^2) \cdot P \cdot R}{(\beta^2 \cdot P)+R}$$
$\beta$大于1时，查全率有更大影响；$\beta$小于1时，查准率有更大影响；$\beta$等于0时，即标准的F1度量。
但是，当正负样本分布发生变化时，P-R曲线会受到较大影响。试想负样本的数量扩大10倍，FP和TN都会成倍增加，会影响到查准率和查全率。这时候我们可以用ROC曲线来评价模型。
定义：
* $TPR = \frac{TP}{TP+FN}$
* $FPR = \frac{FP}{TN+FP}$
绘图的过程可以将样本的预测概率排序，将分类阈值从最小向最大移动，则每个阈值会得到一组TPR和FPR的位置点，将所有的点连结，则绘制成ROC曲线，同时，计算曲线下的面积，即AUC值。AUC值越大，则说明预测的效果越好。<br>

# 代码实现

#### 1.先尝试调用sklearn的线性回归模型训练数据，尝试以下代码，画图查看分类的结果

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

%matplotlib inline
df_X = pd.read_csv('./logistic_x.txt', sep='\ +',header=None, engine='python') #读取X值
ys = pd.read_csv('./logistic_y.txt', sep='\ +',header=None, engine='python') #读取y值
ys = ys.astype(int)
df_X['label'] = ys[0].values #将X按照y值的结果一一打标签
ax = plt.axes()
#在二维图中描绘X点所处位置，直观查看数据点的分布情况
df_X.query('label == 0').plot.scatter(x=0, y=1, ax=ax, color='blue')
df_X.query('label == 1').plot.scatter(x=0, y=1, ax=ax, color='red')
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200114222252217.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc5OTQxOQ==,size_16,color_FFFFFF,t_70)

```
#提取用于学习的数据
Xs = df_X[[0, 1]].values
Xs = np.hstack([np.ones((Xs.shape[0], 1)), Xs]) 
ys = df_X['label'].values
from __future__ import print_function
import numpy as np
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(fit_intercept=False) #因为前面已经将截距项的值合并到变量中，此处参数设置不需要截距项
lr.fit(Xs, ys) #拟合
score = lr.score(Xs, ys) #结果评价
print("Coefficient: %s" % lr.coef_)
print("Score: %s" % score)
ax = plt.axes()

df_X.query('label == 0').plot.scatter(x=0, y=1, ax=ax, color='blue')
df_X.query('label == 1').plot.scatter(x=0, y=1, ax=ax, color='red')

_xs = np.array([np.min(Xs[:,1]), np.max(Xs[:,1])])

#将数据以二维图形式描点，并用学习得出的参数结果作为阈值，划分数据区域
_ys = (lr.coef_[0][0] + lr.coef_[0][1] * _xs) / (- lr.coef_[0][2])
plt.plot(_xs, _ys, lw=1
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200114222305303.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzc5OTQxOQ==,size_16,color_FFFFFF,t_70)
#### 2. 用梯度下降法将相同的数据分类，画图和sklearn的结果相比较

```
class LGR_GD():
    def __init__(self):
        self.w = None 
        self.n_iters = None
    def fit(self,X,y,alpha=0.03,loss = 1e-10): # 设定步长为0.002，判断是否收敛的条件为1e-10
        y = y.reshape(-1,1) #重塑y值的维度以便矩阵运算
        [m,d] = np.shape(X) #自变量的维度
        self.w = np.zeros((1,d)) #将参数的初始值定为0
        tol = 1e5
        self.n_iters = 0
  
        while tol > loss: #设置收敛条件
            h=1/(1+np.exp(-X.dot(self.w.T)))
            self.w = self.w + alpha*np.mean(X*(y-h),axis=0)
            tol = np.abs(np.sum(h))
            self.n_iters += 1 #更新迭代次数
   
    def predict(self, X):
        # 用已经拟合的参数值预测新自变量
        y_pred = X.dot(self.w)
        return y_pred  

if __name__ == "__main__":
    lr_gd = LGR_GD()
    lr_gd.fit(Xs,ys)

    ax = plt.axes()

    df_X.query('label == 0').plot.scatter(x=0, y=1, ax=ax, color='blue')
    df_X.query('label == 1').plot.scatter(x=0, y=1, ax=ax, color='red')

    _xs = np.array([np.min(Xs[:,1]), np.max(Xs[:,1])])
    _ys = (lr_gd.w[0][0] + lr_gd.w[0][1] * _xs) / (- lr_gd.w[0][2])
    plt.plot(_xs, _ys, lw=1)
```
#### 3. 用牛顿法实现结果，画图和sklearn的结果相比较，并比较牛顿法和梯度下降法迭代收敛的次数

```
class LGR_NT():
    def __init__(self):
        self.w = None
        self.n_iters = None
    def fit(self,X,y,loss = 1e-10): # 判断是否收敛的条件为1e-10
        y = y.reshape(-1,1) #重塑y值的维度以便矩阵运算
        [m,d] = np.shape(X) #自变量的维度
        self.w = np.zeros((1,d)) #将参数的初始值定为0
        tol = 1e5
        n_iters =0
        Hessian = np.zeros((d,d))

        while tol > loss:
            tol > loss:
            zs = X.dot(self.w.T)
            h_x = 1 / (1 + np.exp(-zs))
            grad = np.mean(X*(y - h_x),axis=0)
            for i in range(d):
                for j in range(d):
                    if j>=i:
                        Hessian[i][j] = np.mean(h_x*(h_x-1)*X[:,i]*X[:,j]) #更新海森矩阵中的值
                    else:
                        Hessian[i][j] = Hessian[j][i] #按海森矩阵的性质，对称点可直接得出结果
            theta = self.w - np.linalg.inv(Hessian).dot(grad)
            tol = np.sum(np.abs(theta - self.w))
            self.w = theta
            n_iters += 1
    
        self.w = theta
        self.n_iters = n_iters
        
    def predict(self, X):
        # 用已经拟合的参数值预测新自变量
        y_pred = X.dot(self.w)
        return y_pred  

if __name__ == "__main__":
    lgr_nt = LGR_NT()
    lgr_nt.fit(Xs,ys)
```
